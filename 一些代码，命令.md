```python
"""
python 命令行参数
"""
parser = argparse.ArgumentParser()
parser.add_argument('--scity', type=str, default='NY')
```

```python
"""
numpy.sum()
"""
a = np.linspace(1,20,20).reshape(4,5)
print(a)

输出:
[[ 1.  2.  3.  4.  5.]
 [ 6.  7.  8.  9. 10.]
 [11. 12. 13. 14. 15.]
 [16. 17. 18. 19. 20.]]

如果axis默认/缺失
np.sum(a)
即将数组/矩阵中的元素全部加起来，得到一个和：210

如果axis为整数，axis的取值不可大于数组/矩阵的维度，且axis的不同取值会产生不同的结果。
np.sum(a,axis = 0)
axis为0是压缩行,即将每一列的元素相加,将矩阵压缩为一行，
输出：array([34., 38., 42., 46., 50.])

np.sum(a,axis = 1)
axis为1是压缩列,即将每一行的元素相加,将矩阵压缩为一列，
输出：array([15., 40., 65., 90.])

当axis取负数的时候，对于二维矩阵，只能取-1和-2（不可超过矩阵的维度）。
当axis=-1时，相当于axis=1的效果，当axis=-2时，相当于axis=0的效果。

如果axis为整数元组（x，y），则是求出axis=x和axis=y情况下得到的和。
np.sum(a,axis = (0,1))
输出：210

```

```python
"""
numpy.reshape()
"""

numpy.reshape(a, newshape, order='C')
在不更改其数据的情况下为数组赋予新形状。
Parameters

    a array_like
    要被改变形状的列表

    newshape int or tuple of ints
    新形状应与原始形状兼容。如果是整数，则结果将是该长度的一维数组。一个形状维度可以是-1。在这种情况下，该值是从数组的长度和剩余维度推断出来的。

    order {‘C’, ‘F’, ‘A’}, optional
    使用此索引顺序读取 a 的元素，并使用此索引顺序将元素放入重构的数组中。 ‘C’ 表示使用类似 C 的索引顺序读取/写入元素，最后一个轴索引变化最快，回到第一个轴索引变化最慢。 “F”表示使用类似 Fortran 的索引顺序读取/写入元素，第一个索引变化最快，最后一个索引变化最慢。请注意，“C”和“F”选项不考虑底层数组的内存布局，只考虑索引的顺序。如果 a 在内存中是 Fortran 连续的，则“A”表示以类似 Fortran 的索引顺序读取/写入元素，否则以类似 C 的顺序读取/写入元素。


Returns
如果可能，这将是一个新的视图对象；否则，它将是一个副本。请注意，不保证返回数组的内存布局（C 或 Fortran 连续）。
```

```python
"""
numpy.argsort()
"""
numpy.argsort(a, axis=- 1, kind=None, order=None)
返回将对数组进行排序的索引。

使用 kind 关键字指定的算法沿给定轴执行间接排序。它返回一个索引数组，其形状与索引数据沿给定轴按排序顺序排列。

Parameters
a array_like
Array to sort.

axis int or None, optional
要排序的轴。默认值为 -1（最后一个轴）。如果为 None，则使用展平数组。

kind {‘quicksort’, ‘mergesort’, ‘heapsort’, ‘stable’}, optional
排序算法。默认为“快速排序”。请注意，“stable”和“mergesort”都在后台使用 timsort，通常，实际实现会因数据类型而异。保留“mergesort”选项是为了向后兼容。
Changed in version 1.15.0.: The ‘stable’ option was added.

order str or list of str, optional
When a is an array with fields defined, this argument specifies which fields to compare first, second, etc. A single field can be specified as a string, and not all fields need be specified, but unspecified fields will still be used, in the order in which they come up in the dtype, to break ties.

Returns
沿指定轴对 a 进行排序的索引数组。如果 a 是一维的，则 a[index_array] 产生一个已排序的 a。更一般地说， np.take_along_axis(a, index_array, axis=axis) 总是产生排序后的 a，与维度无关。
i.e.
One dimensional array:

    x = np.array([3, 1, 2])
    np.argsort(x)
    array([1, 2, 0])
```

```python
"""
python slice
"""
# 负号表示从倒数方向数，
# i.e.
a = [12, 3, 4, 5, 6, 7, 8]
c, d = a[-2:], a[:-2]
print(c)
print(d)
[7, 8]
[12, 3, 4, 5, 6]
```

```python
"""
numpy.nonzero()
"""
numpy.nonzero(a)
Return the indices of the elements that are non-zero.

Returns a tuple of arrays, one for each dimension of a, containing the indices of the non-zero elements in that dimension. The values in a are always tested and returned in row-major, C-style order.

i.e.
两个列表就是x坐标序列和y坐标序列
x = np.array([[3, 0, 0], [0, 4, 0], [5, 6, 0]])
x
array([[3, 0, 0],
       [0, 4, 0],
       [5, 6, 0]])
np.nonzero(x)
(array([0, 1, 2, 2]), array([0, 1, 0, 1]))
x[np.nonzero(x)]
array([3, 4, 5, 6])
np.transpose(np.nonzero(x))
array([[0, 0],
       [1, 1],
       [2, 0],
       [2, 1]])
```

```python
"""
torch.transpose
"""
torch.transpose(input, dim0, dim1) → Tensor
Returns a tensor that is a transposed version of input. The given dimensions dim0 and dim1 are swapped.

If input is a strided tensor then the resulting out tensor shares its underlying storage with the input tensor, so changing the content of one would change the content of the other.

If input is a sparse tensor then the resulting out tensor does not share the underlying storage with the input tensor.

Parameters
input (Tensor) – the input tensor.

dim0 (int) – the first dimension to be transposed

dim1 (int) – the second dimension to be transposed

Example:

>>> x = torch.randn(2, 3)
>>> x
tensor([[ 1.0028, -0.9893,  0.5809],
        [-0.1669,  0.7299,  0.4942]])
>>> torch.transpose(x, 0, 1)
tensor([[ 1.0028, -0.1669],
        [-0.9893,  0.7299],
        [ 0.5809,  0.4942]])
```

```python
"""
torch.matmul
"""
torch.matmul(input, other, *, out=None) → Tensor
Matrix product of two tensors.

The behavior depends on the dimensionality of the tensors as follows:

If both tensors are 1-dimensional, the dot product (scalar) is returned.

If both arguments are 2-dimensional, the matrix-matrix product is returned.

If the first argument is 1-dimensional and the second argument is 2-dimensional, a 1 is prepended to its dimension for the purpose of the matrix multiply. After the matrix multiply, the prepended dimension is removed.

If the first argument is 2-dimensional and the second argument is 1-dimensional, the matrix-vector product is returned.

This operator supports TensorFloat32.

On certain ROCm devices, when using float16 inputs this module will use different precision for backward.

Parameters
input (Tensor) – the first tensor to be multiplied

other (Tensor) – the second tensor to be multiplied

Keyword Arguments
out (Tensor, optional) – the output tensor.

Example:

>>> # vector x vector
>>> tensor1 = torch.randn(3)
>>> tensor2 = torch.randn(3)
>>> torch.matmul(tensor1, tensor2).size()
torch.Size([])
>>> # matrix x vector
>>> tensor1 = torch.randn(3, 4)
>>> tensor2 = torch.randn(4)
>>> torch.matmul(tensor1, tensor2).size()
torch.Size([3])
>>> # batched matrix x broadcasted vector
>>> tensor1 = torch.randn(10, 3, 4)
>>> tensor2 = torch.randn(4)
>>> torch.matmul(tensor1, tensor2).size()
torch.Size([10, 3])
>>> # batched matrix x batched matrix
>>> tensor1 = torch.randn(10, 3, 4)
>>> tensor2 = torch.randn(10, 4, 5)
>>> torch.matmul(tensor1, tensor2).size()
torch.Size([10, 3, 5])
>>> # batched matrix x broadcasted matrix
>>> tensor1 = torch.randn(10, 3, 4)
>>> tensor2 = torch.randn(4, 5)
>>> torch.matmul(tensor1, tensor2).size()
torch.Size([10, 3, 5])
```

```python
"""
pytorch 多个网络参数一起学习
"""
emb_param_list = list(mvgat.parameters()) + list(fusion.parameters()) + list(edge_disc.parameters())
emb_optimizer = optim.Adam(emb_param_list, lr=args.learning_rate, weight_decay=args.weight_decay)
```

```python
"""
torch.mean()
"""
torch.mean(input, dim, keepdim=False, *, dtype=None, out=None) → Tensor
返回给定维度 dim 中输入张量的每一行的平均值。如果 dim 是维度列表，则对所有维度进行归约。

如果 keepdim 为 True，则输出张量的大小与输入相同，除了尺寸为 1 的维度 dim 1 (或 len(dim)) 更少的维度。

Parameters
input (Tensor) – the input tensor.

dim (int or tuple of python:ints) – the dimension or dimensions to ||reduce.

keepdim (bool) – whether the output tensor has dim retained or not.

Keyword Arguments
dtype (torch.dtype, optional) – the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.

out (Tensor, optional) – the output tensor.

Example:

>>> a = torch.randn(4, 4)
>>> a
tensor([[-0.3841,  0.6320,  0.4254, -0.7384],
        [-0.9644,  1.0131, -0.6549, -1.4279],
        [-0.2951, -1.3350, -0.7694,  0.5600],
        [ 1.0842, -0.9580,  0.3623,  0.2343]])
>>> torch.mean(a, 1) # 横向求平均
tensor([-0.0163, -0.5085, -0.4599,  0.1807])
>>> torch.mean(a, 1, True)
tensor([[-0.0163],
        [-0.5085],
        [-0.4599],
        [ 0.1807]])

```

[Pytorch autograd,backward详解](https://zhuanlan.zhihu.com/p/83172023)
```python
"""
Pytorch autograd,backward
"""
Pytorch中所有的计算其实都可以回归到Tensor上，所以有必要重新认识一下Tensor。如果我们需要计算某个Tensor的导数，那么我们需要设置其.requires_grad属性为True。为方便说明，在本文中对于这种我们自己定义的变量，我们称之为叶子节点(leaf nodes)，而基于叶子节点得到的中间或最终变量则可称之为结果节点。例如下面例子中的x则是叶子节点，y则是结果节点。

x = torch.rand(3, requires_grad=True)
y = x**2
z = x + x
另外一个Tensor中通常会记录如下图中所示的属性：

data: 即存储的数据信息
requires_grad: 设置为True则表示该Tensor需要求导
grad: 该Tensor的梯度值，每次在计算backward时都需要将前一时刻的梯度归零，否则梯度值会一直累加，这个会在后面讲到。
grad_fn: 叶子节点通常为None，只有结果节点的grad_fn才有效，用于指示梯度函数是哪种类型。例如上面示例代码中的y.grad_fn=<PowBackward0 at 0x213550af048>, z.grad_fn=<AddBackward0 at 0x2135df11be0>
is_leaf: 用来指示该Tensor是否是叶子节点。
torch.autograd.backward
有如下代码：

x = torch.tensor(1.0, requires_grad=True)
y = torch.tensor(2.0, requires_grad=True)
z = x**2+y
z.backward()
print(z, x.grad, y.grad)

>>> tensor(3., grad_fn=<AddBackward0>) tensor(2.) tensor(1.)
可以z是一个标量，当调用它的backward方法后会根据链式法则自动计算出叶子节点的梯度值。

但是如果遇到z是一个向量或者是一个矩阵的情况，这个时候又该怎么计算梯度呢？这种情况我们需要定义grad_tensor来计算矩阵的梯度。在介绍为什么使用之前我们先看一下源代码中backward的接口是如何定义的：

torch.autograd.backward(
		tensors, 
		grad_tensors=None, 
		retain_graph=None, 
		create_graph=False, 
		grad_variables=None)
tensor: 用于计算梯度的tensor。也就是说这两种方式是等价的：torch.autograd.backward(z) == z.backward()
grad_tensors: 在计算矩阵的梯度时会用到。他其实也是一个tensor，shape一般需要和前面的tensor保持一致。
retain_graph: 通常在调用一次backward后，pytorch会自动把计算图销毁，所以要想对某个变量重复调用backward，则需要将该参数设置为True
create_graph: 当设置为True的时候可以用来计算更高阶的梯度
grad_variables: 这个官方说法是grad_variables' is deprecated. Use 'grad_tensors' instead.也就是说这个参数后面版本中应该会丢弃，直接使用grad_tensors就好了。
好了，参数大致作用都介绍了，下面我们看看pytorch为什么设计了grad_tensors这么一个参数，以及它有什么用呢？

还是用代码做个示例

x = torch.ones(2,requires_grad=True)
z = x + 2
z.backward()

>>> ...
RuntimeError: grad can be implicitly created only for scalar outputs
当我们运行上面的代码的话会报错，报错信息为RuntimeError: grad can be implicitly created only for scalar outputs。

上面的报错信息意思是只有对标量输出它才会计算梯度，而求一个矩阵对另一矩阵的导数束手无策。
那么我们只要想办法把矩阵转变成一个标量不就好了？比如我们可以对z求和，然后用求和得到的标量在对x求导，这样不会对结果有影响我们可以看到对z求和后再计算梯度没有报错，结果也与预期一样：

x = torch.ones(2,requires_grad=True)
z = x + 2
z.sum().backward()
print(x.grad)

>>> tensor([1., 1.])
我们再仔细想想，对z求和不就是等价于z 点乘一个相同维度的全为1的矩阵吗？即  ,而这个I也就是我们需要传入的grad_tensors参数。(点乘只是相对于一维向量而言的，对于矩阵或更高为的张量，可以看做是对每一个维度做点乘)

代码如下：

x = torch.ones(2,requires_grad=True)
z = x + 2
z.backward(torch.ones_like(z)) # grad_tensors需要与输入tensor大小一致
print(x.grad)

>>> tensor([1., 1.])
```

```python
"""
torch.autograd.grad
"""
TORCH.AUTOGRAD.GRAD
torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False, is_grads_batched=False)
计算并返回输出相对于输入的梯度总和。

grad_outputs 应该是长度匹配输出的序列，其中包含向量-雅可比积中的“向量”，通常是预先计算的 w.r.t 梯度。每个输出。如果输出不需要_grad，则梯度可以是 None)。
```

```python
"""
torch.nn.utils.clip_grad_norm_
"""
torch.nn.utils.clip_grad_norm_(parameters, max_norm, norm_type=2.0, error_if_nonfinite=False)
裁剪参数迭代的梯度范数。

范数是在所有梯度上一起计算的，就好像它们被连接成一个向量一样。渐变被原地修改
```

```python
"""
__future__
"""
我们在读代码的时候，总是会看到代码开头会加上from __future__ import *这样的语句。这样的做法的作用就是将新版本的特性引进当前版本中，也就是说我们可以在当前版本使用新版本的一些特性。

例如，在python2.x和python3.x中print的标准写法分别是，

# python 2.x
print "Hello World"
 
# python 3.x
print("Hello World")
如果你想用python2.x体验python3.x的写法，就可以使用from __future__ import print_function来实现，

# python 2.x
from __future__ import print_function
print("Hello World")
```

```python
"""
TORCH.QUANTILE
"""

torch.quantile(input, q, dim=None, keepdim=False, *, interpolation='linear', out=None) → Tensor
Computes the q-th quantiles of each row of the input tensor along the dimension dim.

To compute the quantile, we map q in [0, 1] to the range of indices [0, n] to find the location of the quantile in the sorted input. If the quantile lies between two data points a < b with indices i and j in the sorted order, result is computed according to the given interpolation method as follows:

linear: a + (b - a) * fraction, where fraction is the fractional part of the computed quantile index.

lower: a.

higher: b.

nearest: a or b, whichever’s index is closer to the computed quantile index (rounding down for .5 fractions).

midpoint: (a + b) / 2.

If q is a 1D tensor, the first dimension of the output represents the quantiles and has size equal to the size of q, the remaining dimensions are what remains from the reduction.

NOTE

By default dim is None resulting in the input tensor being flattened before computation.

Parameters
input (Tensor) – the input tensor.

q (float or Tensor) – a scalar or 1D tensor of values in the range [0, 1].

dim (int) – the dimension to reduce.

keepdim (bool) – whether the output tensor has dim retained or not.

Example:

>>> a = torch.randn(2, 3)
>>> a
tensor([[ 0.0795, -1.2117,  0.9765],
        [ 1.1707,  0.6706,  0.4884]])
>>> q = torch.tensor([0.25, 0.5, 0.75])
>>> torch.quantile(a, q, dim=1, keepdim=True)
tensor([[[-0.5661],
        [ 0.5795]],

        [[ 0.0795],
        [ 0.6706]],

        [[ 0.5280],
        [ 0.9206]]])

```

```python
"""
jupyter notebook
"""
[notebook 中使用args](https://zhuanlan.zhihu.com/p/145720581)

```

```python
"""
归一化
"""
import torch
import torch.nn.functional as F
a = torch.arange(9, dtype= torch.float)
a = a.reshape((3,3))
print(a)
'''
tensor([[0., 1., 2.],
        [3., 4., 5.],
        [6., 7., 8.]])
'''
# 对二维数组按行归一化
# p=2表示二范式, dim=1表示按行归一化
b = F.normalize(a, p=2, dim=1)
print(b)
'''
tensor([[0.0000, 0.4472, 0.8944],
        [0.4243, 0.5657, 0.7071],
        [0.4915, 0.5735, 0.6554]])
'''

```

```python
"""
cos 相似性与向量取模
"""
# 内积
s = torch.inner(fused_emb_s[0], fused_emb_t[th_mask_target.view(-1).bool()].sum(0).reshape(1,64))
# 取模
s = s / fused_emb_s[0].pow(2).sum().pow(1/2)
s = s / fused_emb_t[th_mask_target.view(-1).bool()].sum(0).reshape(1,64).pow(2).sum().pow(1/2)
log(s)
```

```bash
# 导出requirements.txt
pip install pipreqs
pipreqs . --encoding="utf-8"

# 默认源
pip install -i https://pypi.org/simple dgl_cuda11.3==0.9.0

# 清华源
-i https://pypi.tuna.tsinghua.edu.cn/simple some-package

# conda create
conda create -n "" python=3.8

# proxy
set http_proxy=http://127.0.0.1:33210
set https_proxy=http://127.0.0.1:33210

# 切换多城市分支
git checkout "feature_multi_source"

```
